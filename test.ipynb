{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "83e0bf92",
      "metadata": {
        "id": "83e0bf92"
      },
      "source": [
        "## Create intervene dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f794fb56",
      "metadata": {
        "id": "f794fb56"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "from tqdm import tqdm, trange\n",
        "random.seed(42)\n",
        "\n",
        "data_path = \"data/longmemeval/longmemeval_s.json\"\n",
        "with open(data_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "sample_num_for_each_data = 1\n",
        "intervene_data_num = 2\n",
        "\n",
        "# sample set of data to intervene\n",
        "def sample_intervene_data(data_len, sample_num_for_each_data, intervene_data_num):\n",
        "    intervene_list = []\n",
        "    sample_data_num = intervene_data_num-1\n",
        "    for i in range(data_len):\n",
        "        buffer = []\n",
        "        pool = [j for j in range(data_len) if j != i]\n",
        "        for _ in range(sample_num_for_each_data):\n",
        "            random.shuffle(pool)\n",
        "            sampled_list = [i]+pool[:sample_data_num]\n",
        "            buffer.append(tuple(sampled_list))\n",
        "        intervene_list+=buffer\n",
        "    return tuple(intervene_list)\n",
        "\n",
        "intervene_haystack_idx = sample_intervene_data(len(data), sample_num_for_each_data, intervene_data_num)\n",
        "# shape of intervene_haystack_idx = (len(data) * sample_num_for_each_data, intervene_data_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a549abf",
      "metadata": {
        "id": "6a549abf"
      },
      "outputs": [],
      "source": [
        "data[0][\"haystack_sessions\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45121b32",
      "metadata": {
        "id": "45121b32"
      },
      "outputs": [],
      "source": [
        "sample_session_num = 2\n",
        "# Sample sessions to intervene for each data\n",
        "def sample_session(data, intervene_haystack_idx, sample_session_num):\n",
        "    intervene_sampled_session_data = []\n",
        "    for haystack_group in tqdm(intervene_haystack_idx):\n",
        "        buffer = []\n",
        "        for haystack_idx in haystack_group:\n",
        "            temp = {\"idx\": haystack_idx, \"session_idx\": []}\n",
        "            cur_data_session_ids = data[haystack_idx][\"haystack_session_ids\"]\n",
        "            pool = [i for i in range(len(cur_data_session_ids))]\n",
        "            # first get the index of session that contains the answer in its id\n",
        "            for session_idx in range(len(cur_data_session_ids)):\n",
        "                if \"answer\" in cur_data_session_ids[session_idx]: # TODO: Can change this to direct check with \"answer_session_ids\"\n",
        "                    temp[\"session_idx\"].append(session_idx)\n",
        "                    pool.remove(session_idx)\n",
        "            # then sample the rest session_idx from the available sessions\n",
        "            cur_sample_num = sample_session_num - len(temp[\"session_idx\"])\n",
        "            if cur_sample_num > 0:\n",
        "                sampled_session_idx = random.sample(pool, cur_sample_num)\n",
        "                temp[\"session_idx\"].extend(sampled_session_idx)\n",
        "            # sort the session_idx\n",
        "            temp[\"session_idx\"] = temp[\"session_idx\"][:sample_session_num]\n",
        "            temp[\"session_idx\"].sort()\n",
        "            buffer.append(temp)\n",
        "        intervene_sampled_session_data.append(tuple(buffer))\n",
        "    return intervene_sampled_session_data\n",
        "\n",
        "intervene_sampled_session_data = sample_session(data, intervene_haystack_idx[:10], sample_session_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c188d5e1",
      "metadata": {
        "id": "c188d5e1"
      },
      "outputs": [],
      "source": [
        "# concatenate the session data for each data pair\n",
        "def retrieve_session_data(data, haystack_group):\n",
        "    def get_string_from_session(session):\n",
        "        buffer = \"\"\n",
        "        for message in session:\n",
        "            buffer+= f\"{message['role']}: {message['content']}\\n\\n\"\n",
        "        return buffer.strip()\n",
        "\n",
        "    buffer = \"\"\n",
        "    haystack_idx_list = [item[\"idx\"] for item in haystack_group]\n",
        "    session_idx_list = [item[\"session_idx\"] for item in haystack_group]\n",
        "    for cur_session_idx in range(len(session_idx_list[0])):\n",
        "        for cur_haystack_idx in range(len(haystack_idx_list)):\n",
        "            haystack_idx = haystack_idx_list[cur_haystack_idx]\n",
        "            session_idx = session_idx_list[cur_haystack_idx][cur_session_idx]\n",
        "            # print(f\"haystack_idx: {haystack_idx}, session_idx: {session_idx}\")\n",
        "            buffer += get_string_from_session(data[haystack_idx][\"haystack_sessions\"][session_idx])+\"\\n\\n\\n\\n\"\n",
        "    return buffer.strip()\n",
        "\n",
        "\n",
        "print(intervene_sampled_session_data[0])\n",
        "retrieve_session_data(data, intervene_sampled_session_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d253c604",
      "metadata": {
        "id": "d253c604"
      },
      "source": [
        "## test model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccf00c52",
      "metadata": {
        "id": "ccf00c52"
      },
      "source": [
        "### flan-t5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db9a9c21",
      "metadata": {
        "id": "db9a9c21"
      },
      "outputs": [],
      "source": [
        "# load flan-T5 for prototyping\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abdf1e26",
      "metadata": {
        "id": "abdf1e26"
      },
      "source": [
        "### Llama-3.1-8B-Instruct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85346272",
      "metadata": {
        "id": "85346272"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.save_pretrained(\"/content/drive/MyDrive/Research/pretrained_models/llama-3.1-8B\")\n",
        "\n",
        "def generate_response(prompt, input):\n",
        "    pipeline = transformers.pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model_id,\n",
        "        model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": prompt},\n",
        "        {\"role\": \"user\", \"content\": input},\n",
        "    ]\n",
        "\n",
        "    outputs = pipeline(\n",
        "        messages,\n",
        "        max_new_tokens=256,\n",
        "    )\n",
        "    return outputs[0][\"generated_text\"][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K-eQ9u9mtNpC",
      "metadata": {
        "id": "K-eQ9u9mtNpC"
      },
      "outputs": [],
      "source": [
        "generate_response(\"You are a pirate chatbot who always responds in pirate speak!\", \"Who are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33a6f504",
      "metadata": {
        "id": "33a6f504"
      },
      "source": [
        "### prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b4c571f",
      "metadata": {
        "id": "0b4c571f"
      },
      "outputs": [],
      "source": [
        "# prompts\n",
        "extraction_prompt = \"\"\"You are now an intelligent assistant tasked with meticulously extracting both key elements and atomic facts from a long text.\n",
        "1. Key Elements: The essential nouns (e.g., characters, times, events, places, numbers), verbs (e.g., actions), and adjectives (e.g., states, feelings) that are pivotal to the text’s narrative.\n",
        "2. Atomic Facts: The smallest, indivisible facts, presented as concise sentences. These include propositions, theories, existences, concepts, and implicit elements like logic, causality, event sequences, interpersonal relationships, timelines, etc.\n",
        "\n",
        "Requirements:\n",
        "#####\n",
        "1. Ensure that all identified key elements are reflected within the corresponding atomic facts.\n",
        "2. You should extract key elements and atomic facts comprehensively, especially those that are important and potentially query-worthy and do not leave out details.\n",
        "3. Whenever applicable, replace pronouns with their specific noun counterparts (e.g., change I, He, She to actual names).\n",
        "4. Ensure that the key elements and atomic facts you extract are presented in the same language as the original text (e.g., English or Chinese).\n",
        "5. You should output a total of key elements and atomic facts that do not exceed 1024 tokens.\n",
        "6. Your answer format for each line should be: [Serial Number], [Atomic Facts], [List of Key Elements, separated with ‘|’]\n",
        "#####\n",
        "\n",
        "Example:\n",
        "#####\n",
        "User:\n",
        "One day, a father and his little son ......\n",
        "Assistant:\n",
        "1. One day, a father and his little son were going home. | father | little son | going home\n",
        "2. ......\n",
        "#####\n",
        "\n",
        "Please strictly follow the above format. Let’s begin.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "rational_plan_prompt = \"\"\"As an intelligent assistant, your primary objective is to answer the question by gathering supporting facts from a given article. To facilitate this objective, the first step is to make a rational plan based on the question. This plan should outline the step-by-step process to resolve the question and specify the key information required to formulate a comprehensive answer.\n",
        "\n",
        "Example:\n",
        "#####\n",
        "User: Who had a longer tennis career, Danny or Alice?\n",
        "\n",
        "Assistant: In order to answer this question, we first need to find the length of Danny’s and Alice’s tennis careers, such as the start and retirement of their careers, and then compare the two.\n",
        "#####\n",
        "\n",
        "Please strictly follow the above format. Let’s begin.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "initial_node_prompt = \"\"\"As an intelligent assistant, your primary objective is to answer questions based on information contained within a text. To facilitate this objective, a graph has been created from the text, comprising the following elements:\n",
        "1. Text Chunks: Chunks of the original text.\n",
        "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
        "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic facts derived from different text chunks.\n",
        "\n",
        "Your current task is to check a list of nodes, with the objective of selecting the most relevant initial nodes from the graph to efficiently answer the question. You are given the question, the rational plan, and a list of node key elements. These initial nodes are crucial because they are the starting point for searching for relevant information.\n",
        "\n",
        "Requirements:\n",
        "#####\n",
        "1. Once you have selected a starting node, assess its relevance to the potential answer by assigning a score between 0 and 100. A score of 100 implies a high likelihood of relevance to the answer, whereas a score of 0 suggests minimal relevance.\n",
        "2. Present each chosen starting node in a separate line, accompanied by its relevance score. Format each line as follows: Node: [Key Element of Node], Score: [Relevance Score].\n",
        "3. Please select at least 10 starting nodes, ensuring they are non-repetitive and diverse.\n",
        "4. In the user’s input, each line constitutes a node. When selecting the starting node, please make your choice from those provided, and refrain from fabricating your own. The nodes you output must correspond exactly to the nodes given by the user, with identical wording.\n",
        "#####\n",
        "\n",
        "Example:\n",
        "#####\n",
        "User:\n",
        "Question: {QUESTION}\n",
        "Plan: {RATIONAL PLAN}\n",
        "Nodes: {LIST OF KEY ELEMENTS}\n",
        "\n",
        "Assistant:{LIST OF SELECTED NODES}\n",
        "#####\n",
        "\n",
        "Finally, I emphasize again that you need to select the starting node from the given Nodes, and it must be consistent with the words of the node you selected. Please strictly follow the above format. Let’s begin.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "explore_atomic_prompt = \"\"\"As an intelligent assistant, your primary objective is to answer questions based on information\n",
        "contained within a text. To facilitate this objective, a graph has been created from the text,\n",
        "comprising the following elements:\n",
        "1. Text Chunks: Chunks of the original text.\n",
        "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
        "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
        "facts derived from different text chunks.\n",
        "\n",
        "Your current task is to check a node and its associated atomic facts, with the objective of\n",
        "determining whether to proceed with reviewing the text chunk corresponding to these atomic facts.\n",
        "Given the question, the rational plan, previous actions, notebook content, and the current node’s\n",
        "atomic facts and their corresponding chunk IDs, you have the following Action Options:\n",
        "#####\n",
        "1. read_chunk(List[ID]): Choose this action if you believe that a text chunk linked to an atomic\n",
        "fact may hold the necessary information to answer the question. This will allow you to access\n",
        "more complete and detailed information.\n",
        "2. stop_and_read_neighbor(): Choose this action if you ascertain that all text chunks lack valuable\n",
        "information.\n",
        "#####\n",
        "\n",
        "Strategy:\n",
        "#####\n",
        "1. Reflect on previous actions and prevent redundant revisiting nodes or chunks.\n",
        "2. You can choose to read multiple text chunks at the same time.\n",
        "3. Atomic facts only cover part of the information in the text chunk, so even if you feel that the\n",
        "atomic facts are slightly relevant to the question, please try to read the text chunk to get more\n",
        "complete information.\n",
        "#####\n",
        "\n",
        "Response format:\n",
        "#####\n",
        "*Updated Notebook*: First, combine your current notebook with new insights and findings about\n",
        "the question from current atomic facts, creating a more complete version of the notebook that\n",
        "contains more valid information.\n",
        "*Rationale for Next Action*: Based on the given question, the rational plan, previous actions, and\n",
        "notebook content, analyze how to choose the next action.\n",
        "*Chosen Action*: read_chunk(List[ID]) or stop_and_read_neighbor(). (Here is the Action you\n",
        "selected from Action Options, which is in the form of a function call as mentioned before. The\n",
        "formal parameter in parentheses should be replaced with the actual parameter.)\n",
        "#####\n",
        "\n",
        "Finally, it is emphasized again that even if the atomic fact is only slightly relevant to the\n",
        "question, you should still look at the text chunk to avoid missing information. You should only\n",
        "choose stop_and_read_neighbor() when you are very sure that the given text chunk is irrelevant to\n",
        "the question. Please strictly follow the above format. Let’s begin.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "explore_chunk_prompt = \"\"\"As an intelligent assistant, your primary objective is to answer questions based on information\n",
        "within a text. To facilitate this objective, a graph has been created from the text, comprising the\n",
        "following elements:\n",
        "1. Text Chunks: Segments of the original text.\n",
        "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
        "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
        "facts derived from different text chunks.\n",
        "\n",
        "Your current task is to assess a specific text chunk and determine whether the available information\n",
        "suffices to answer the question. Given the question, rational plan, previous actions, notebook\n",
        "content, and the current text chunk, you have the following Action Options:\n",
        "#####\n",
        "1. search_more(): Choose this action if you think that the essential information necessary to\n",
        "answer the question is still lacking.\n",
        "2. read_previous_chunk(): Choose this action if you feel that the previous text chunk contains\n",
        "valuable information for answering the question.\n",
        "3. read_subsequent_chunk(): Choose this action if you feel that the subsequent text chunk contains\n",
        "valuable information for answering the question.\n",
        "4. termination(): Choose this action if you believe that the information you have currently obtained\n",
        "is enough to answer the question. This will allow you to summarize the gathered information and\n",
        "provide a final answer.\n",
        "#####\n",
        "\n",
        "Strategy:\n",
        "#####\n",
        "1. Reflect on previous actions and prevent redundant revisiting of nodes or chunks.\n",
        "2. You can only choose one action.\n",
        "#####\n",
        "\n",
        "Response format:\n",
        "#####\n",
        "*Updated Notebook*: First, combine your previous notes with new insights and findings about the\n",
        "question from current text chunks, creating a more complete version of the notebook that contains\n",
        "more valid information.\n",
        "*Rationale for Next Action*: Based on the given question, rational plan, previous actions, and\n",
        "notebook content, analyze how to choose the next action.\n",
        "*Chosen Action*: search_more() or read_previous_chunk() or read_subsequent_chunk() or\n",
        "termination(). (Here is the Action you selected from Action Options, which is in the form of a\n",
        "function call as mentioned before. The formal parameter in parentheses should be replaced with\n",
        "the actual parameter.)\n",
        "#####\n",
        "\n",
        "Please strictly follow the above format. Let’s begin.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "explore_neighbor_prompt = \"\"\"As an intelligent assistant, your primary objective is to answer questions based on information\n",
        "within a text. To facilitate this objective, a graph has been created from the text, comprising the\n",
        "following elements:\n",
        "1. Text Chunks: Segments of the original text.\n",
        "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
        "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
        "facts derived from different text chunks.\n",
        "\n",
        "Your current task is to assess all neighboring nodes of the current node, with the objective of determining whether to proceed to the next neighboring node. Given the question, rational\n",
        "plan, previous actions, notebook content, and the neighbors of the current node, you have the\n",
        "following Action Options:\n",
        "#####\n",
        "1. read_neighbor_node(key element of node): Choose this action if you believe that any of the\n",
        "neighboring nodes may contain information relevant to the question. Note that you should focus\n",
        "on one neighbor node at a time.\n",
        "2. termination(): Choose this action if you believe that none of the neighboring nodes possess\n",
        "information that could answer the question.\n",
        "#####\n",
        "\n",
        "Strategy:\n",
        "#####\n",
        "1. Reflect on previous actions and prevent redundant revisiting of nodes or chunks.\n",
        "2. You can only choose one action. This means that you can choose to read only one neighbor\n",
        "node or choose to terminate.\n",
        "#####\n",
        "\n",
        "Response format:\n",
        "#####\n",
        "*Rationale for Next Action*: Based on the given question, rational plan, previous actions, and\n",
        "notebook content, analyze how to choose the next action.\n",
        "*Chosen Action*: read_neighbor_node(neighbor_node) or termination(). (Here is the Action you\n",
        "selected from Action Options, which is in the form of a function call as mentioned before. The\n",
        "formal parameter in parentheses should be replaced with the actual parameter.)\n",
        "#####\n",
        "\n",
        "Please strictly follow the above format. Let’s begin.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "QA_prompt = \"\"\"As an intelligent assistant, your primary objective is to answer questions based on information\n",
        "within a text. To facilitate this objective, a graph has been created from the text, comprising the\n",
        "following elements:\n",
        "1. Text Chunks: Segments of the original text.\n",
        "2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n",
        "3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\n",
        "facts derived from different text chunks.\n",
        "\n",
        "You have now explored multiple paths from various starting nodes on this graph, recording key information for each path in a notebook.\n",
        "Your task now is to analyze these memories and reason to answer the question.\n",
        "\n",
        "Strategy:\n",
        "#####\n",
        "1. You should first analyze each notebook content before providing a final answer.\n",
        "2. During the analysis, consider complementary information from other notes and employ a\n",
        "majority voting strategy to resolve any inconsistencies.\n",
        "3. When generating the final answer, ensure that you take into account all available information.\n",
        "#####\n",
        "\n",
        "Example:\n",
        "#####\n",
        "User:\n",
        "Question: Who had a longer tennis career, Danny or Alice?\n",
        "Notebook of different exploration paths:\n",
        "1. We only know that Danny’s tennis career started in 1972 and ended in 1990, but we don’t know\n",
        "the length of Alice’s career.\n",
        "2. ......\n",
        "\n",
        "Assistant:\n",
        "Analyze:\n",
        "The summary of search path 1 points out that Danny’s tennis career is 1990-1972=18 years.\n",
        "Although it does not indicate the length of Alice’s career, the summary of search path 2 finds this\n",
        "information, that is, the length of Alice’s tennis career is 15 years. Then we can get the final\n",
        "answer, that is, Danny’s tennis career is longer than Alice’s.\n",
        "Final answer:\n",
        "Danny’s tennis career is longer than Alice’s.\n",
        "#####\n",
        "\n",
        "Please strictly follow the above format. Let’s begin.\n",
        "\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af8b070",
      "metadata": {
        "id": "1af8b070"
      },
      "source": [
        "## Graph Construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1c60edc",
      "metadata": {
        "id": "f1c60edc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "\n",
        "def extract_key_elements_and_atomic_facts_T5(text, tokenizer, max_tokens_per_chunk=4096, ):\n",
        "    extraction_prompt = \"\"\"You are now an intelligent assistant tasked with meticulously extracting both key elements and atomic facts from a long text.\n",
        "1. Key Elements: The essential nouns (e.g., characters, times, events, places, numbers), verbs (e.g., actions), and adjectives (e.g., states, feelings) that are pivotal to the text’s narrative.\n",
        "2. Atomic Facts: The smallest, indivisible facts, presented as concise sentences. These include propositions, theories, existences, concepts, and implicit elements like logic, causality, event sequences, interpersonal relationships, timelines, etc.\n",
        "\n",
        "Requirements:\n",
        "#####\n",
        "1. Ensure that all identified key elements are reflected within the corresponding atomic facts.\n",
        "2. You should extract key elements and atomic facts comprehensively, especially those that are important and potentially query-worthy and do not leave out details.\n",
        "3. Whenever applicable, replace pronouns with their specific noun counterparts (e.g., change I, He, She to actual names).\n",
        "4. Ensure that the key elements and atomic facts you extract are presented in the same language as the original text (e.g., English or Chinese).\n",
        "5. You should output a total of key elements and atomic facts that do not exceed 1024 tokens.\n",
        "6. Your answer format for each line should be: [Serial Number], [Atomic Facts], [List of Key Elements, separated with ‘|’]\n",
        "#####\n",
        "\n",
        "Example:\n",
        "#####\n",
        "User:\n",
        "One day, a father and his little son ......\n",
        "Assistant:\n",
        "1. One day, a father and his little son were going home. | father | little son | going home\n",
        "2. ......\n",
        "#####\n",
        "\n",
        "Please strictly follow the above format. Let’s begin.\n",
        "\n",
        "\"\"\"\n",
        "    prompt_tokens = tokenizer.encode(extraction_prompt, return_tensors='pt')\n",
        "    tokens = tokenizer.encode(text, return_tensors='pt')\n",
        "    # break text into chunks with max_tokens_per_chunk\n",
        "    max_context_tokens_per_chunk = max_tokens_per_chunk - len(prompt_tokens[0])\n",
        "    token_chunks = []\n",
        "    for i in trange(0, len(tokens[0]), max_context_tokens_per_chunk):\n",
        "        # chuck the tokens but keep the (1, max_context_tokens_per_chunk) shape\n",
        "        chunk = tokens[0][i:i + max_context_tokens_per_chunk].view(1, -1)\n",
        "        if len(chunk) > 0:\n",
        "            token_chunks.append(chunk)\n",
        "\n",
        "\n",
        "    for context_tokens in token_chunks[:2]:\n",
        "        print(f\"Processing chunk with {context_tokens.shape}\")\n",
        "        print(f\"Prompyt tokens: {prompt_tokens.shape}\")\n",
        "        input_tokens = torch.cat((prompt_tokens, context_tokens), dim=1)\n",
        "        print(f\"Input tokens: {input_tokens.shape}\")\n",
        "\n",
        "        outputs = model.generate(input_tokens, max_length=max_tokens_per_chunk)\n",
        "        output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(output_text)\n",
        "\n",
        "\n",
        "    # Placeholder for actual implementation\n",
        "    return \"Extracted Key Elements and Atomic Facts\"\n",
        "\n",
        "def extract_key_elements_and_atomic_facts(text, tokenizer, max_tokens_per_chunk=4096, ):\n",
        "    extraction_prompt = \"\"\"You are now an intelligent assistant tasked with meticulously extracting both key elements and atomic facts from a long text.\n",
        "1. Key Elements: The essential nouns (e.g., characters, times, events, places, numbers), verbs (e.g., actions), and adjectives (e.g., states, feelings) that are pivotal to the text’s narrative.\n",
        "2. Atomic Facts: The smallest, indivisible facts, presented as concise sentences. These include propositions, theories, existences, concepts, and implicit elements like logic, causality, event sequences, interpersonal relationships, timelines, etc.\n",
        "\n",
        "Requirements:\n",
        "#####\n",
        "1. Ensure that all identified key elements are reflected within the corresponding atomic facts.\n",
        "2. You should extract key elements and atomic facts comprehensively, especially those that are important and potentially query-worthy and do not leave out details.\n",
        "3. Whenever applicable, replace pronouns with their specific noun counterparts (e.g., change I, He, She to actual names).\n",
        "4. Ensure that the key elements and atomic facts you extract are presented in the same language as the original text (e.g., English or Chinese).\n",
        "5. You should output a total of key elements and atomic facts that do not exceed 1024 tokens.\n",
        "6. Your answer format for each line should be: [Serial Number], [Atomic Facts], [List of Key Elements, separated with ‘|’]\n",
        "#####\n",
        "\n",
        "Example:\n",
        "#####\n",
        "User:\n",
        "One day, a father and his little son ......\n",
        "Assistant:\n",
        "1. One day, a father and his little son were going home. | father | little son | going home\n",
        "2. ......\n",
        "#####\n",
        "\n",
        "Please strictly follow the above format. Let’s begin.\n",
        "\n",
        "\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "    prompt_tokens = encoding.encode(extraction_prompt)\n",
        "    tokens = encoding.encode(text)\n",
        "    # break text into chunks with max_tokens_per_chunk\n",
        "    max_context_tokens_per_chunk = max_tokens_per_chunk - len(prompt_tokens)\n",
        "    context_chunks = []\n",
        "    for i in trange(0, len(tokens[0]), max_context_tokens_per_chunk):\n",
        "        # chuck the tokens but keep the (1, max_context_tokens_per_chunk) shape\n",
        "        chunk = tokens[i:i + max_context_tokens_per_chunk]\n",
        "        if len(chunk) > 0:\n",
        "            context_chunks.append(encoding.decode(chunk))\n",
        "\n",
        "\n",
        "    for context in context_chunks[:2]:\n",
        "        output_text = generate_response(extraction_prompt,context)\n",
        "        print(output_text)\n",
        "\n",
        "\n",
        "    # Placeholder for actual implementation\n",
        "    return \"Extracted Key Elements and Atomic Facts\"\n",
        "\n",
        "for issd in intervene_sampled_session_data:\n",
        "    context = retrieve_session_data(data, issd)\n",
        "    extract_key_elements_and_atomic_facts(context, tokenizer)\n",
        "\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
